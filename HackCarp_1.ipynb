{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaJethliya/hackcarpathia/blob/master/HackCarp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DTGUQM_KCaZb",
        "outputId": "9326b587-6aa8-4b88-da18-f5a4b6e89383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting assemblyai\n",
            "  Downloading assemblyai-0.38.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: speechrecognition in /usr/local/lib/python3.11/dist-packages (3.14.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: httpx>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from assemblyai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1.10.17 in /usr/local/lib/python3.11/dist-packages (from assemblyai) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.11/dist-packages (from assemblyai) (4.13.1)\n",
            "Requirement already satisfied: websockets>=11.0 in /usr/local/lib/python3.11/dist-packages (from assemblyai) (15.0.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.19.0->assemblyai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.19.0->assemblyai) (1.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.17->assemblyai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.17->assemblyai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.17->assemblyai) (0.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading assemblyai-0.38.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: assemblyai\n",
            "Successfully installed assemblyai-0.38.0\n"
          ]
        }
      ],
      "source": [
        "pip install assemblyai gradio scikit-learn speechrecognition pydub PyPDF2 python-docx pyngrok nest-asyncio fastapi uvicorn python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9XvoLBeovwf",
        "outputId": "1c89e21b-d653-4f07-db82-f8450dfc1309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.30)\n",
            "Requirement already satisfied: llama-index-llms-gemini in /usr/local/lib/python3.11/dist-packages (0.4.14)\n",
            "Collecting llama-index-embeddings-gemini\n",
            "  Downloading llama_index_embeddings_gemini-0.3.2-py3-none-any.whl.metadata (907 bytes)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.30 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.30)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.33)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-gemini) (0.8.4)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-gemini) (10.4.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.26.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.70.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (3.11.15)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (9.1.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.18)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.18.3)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.7.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.69.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.9)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.12 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.26.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (3.2.3)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama-index) (24.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.4.6)\n",
            "Downloading llama_index_embeddings_gemini-0.3.2-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: llama-index-embeddings-gemini\n",
            "Successfully installed llama-index-embeddings-gemini-0.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install -U openai-whisper llama-index llama-index-llms-gemini llama-index-embeddings-gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "j9MWpvsFSO3G",
        "outputId": "0f12cdf3-48f9-4557-a2f1-9db25ceeb9fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-0b200d76b3c0>:11: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
            "  embedding_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
            "<ipython-input-1-0b200d76b3c0>:19: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(api_key=\"AIzaSyDKEfEmnE9C7OcJ7cSE5rEUV2OBtzXgM0I\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "# from llama_index.core.response_schema import ResponseSchema\n",
        "from llama_index.core.base.response.schema import PydanticResponse\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from pydantic import BaseModel\n",
        "\n",
        "embedding_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
        "\n",
        "class ScamAnalysis(BaseModel):\n",
        "    score: float\n",
        "    verdict: str\n",
        "    explanation: str\n",
        "\n",
        "# Configure Gemini\n",
        "llm = Gemini(api_key=\"AIzaSyDKEfEmnE9C7OcJ7cSE5rEUV2OBtzXgM0I\")\n",
        "sllm = llm.as_structured_llm(output_cls=ScamAnalysis)\n",
        "\n",
        "\n",
        "def analyze_text_with_llamaindex(text):\n",
        "    query = f\"\"\"Analyze this below text for phishing/scam indicators:\n",
        "    1. Urgent requests for personal information\n",
        "    2. Suspicious links/attachments\n",
        "    3. Poor grammar/spelling\n",
        "    4. Unusual sender addresses\n",
        "    5. Threats/pressure tactics\n",
        "    6. Unexpected financial requests\n",
        "\n",
        "    Return JSON response with:\n",
        "    - score (0-1 score indicating the likelihood of the text being scam)\n",
        "    - verdict ('phishing' or 'safe')\n",
        "    - explanation (concise reasoning)\n",
        "\n",
        "    text: {text}\"\"\"\n",
        "    response = sllm.complete(query)\n",
        "    print('-----------text-res--------------------------------', response, type(response))\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "def analyze_pdf_with_llamaindex(pdf_path):\n",
        "\n",
        "    # Load and parse PDF\n",
        "    try:\n",
        "        reader = SimpleDirectoryReader(input_files=[pdf_path])\n",
        "        documents = reader.load_data()\n",
        "\n",
        "        # Configure node parser with chunking\n",
        "        node_parser = SimpleNodeParser.from_defaults(\n",
        "            chunk_size=1024,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        nodes = node_parser.get_nodes_from_documents(documents)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"verdict\": \"error\",\n",
        "            \"explanation\": f\"PDF processing failed: {str(e)}\"\n",
        "        }\n",
        "\n",
        "    # Define response schema\n",
        "    # response_schemas = [\n",
        "    #     PydanticResponse(\n",
        "    #         name=\"scam_analysis\",\n",
        "    #         description=\"Scam analysis results\",\n",
        "    #         response_type=ScamAnalysis\n",
        "    #     )\n",
        "    # ]\n",
        "\n",
        "    # Create query engine\n",
        "    index = VectorStoreIndex(nodes, embed_model = embedding_model)\n",
        "    query_engine = index.as_query_engine(\n",
        "        llm=llm,\n",
        "        response_mode=\"compact\",\n",
        "        output_cls=ScamAnalysis\n",
        "    )\n",
        "\n",
        "    # Construct query\n",
        "    query = \"\"\"Analyze this document for phishing/scam indicators:\n",
        "    1. Urgent requests for personal information\n",
        "    2. Suspicious links/attachments\n",
        "    3. Poor grammar/spelling\n",
        "    4. Unusual sender addresses\n",
        "    5. Threats/pressure tactics\n",
        "    6. Unexpected financial requests\n",
        "\n",
        "    Return JSON response with:\n",
        "    - score (0-1 score indicating the likelihood of the text being scam)\n",
        "    - verdict ('phishing' or 'safe')\n",
        "    - explanation (concise reasoning)\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = query_engine.query(query)\n",
        "        return response.response.dict()\n",
        "        return json.dumps(response.scam_analysis.dict())\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"score\": 0,\n",
        "            \"verdict\": \"error\",\n",
        "            \"explanation\": f\"Analysis failed: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Usage\n",
        "# result = analyze_pdf_with_llamaindex(\"/content/BT_Fraud - Copy.pdf\")\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5CSvmDkCeXB"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import speech_recognition as sr\n",
        "import os\n",
        "# import PyPDF2\n",
        "# import docx\n",
        "import assemblyai as aai\n",
        "\n",
        "\n",
        "# Sample training data (replace with real scam/legit data)\n",
        "# train_texts = [\n",
        "#     \"Urgent! Your account will be closed. Verify now at scam-link.com\",\n",
        "#     \"Congratulations! You won $1,000,000. Click to claim\",\n",
        "#     \"Your package delivery update: tracking number XYZ123\",\n",
        "#     \"Meeting reminder: Tomorrow at 2 PM in conference room\",\n",
        "#     \"Please reset your password following this secure link\"\n",
        "# ]\n",
        "# train_labels = [1, 1, 0, 0, 0]  # 1 = scam, 0 = legitimate\n",
        "\n",
        "# Train simple classifier\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# X_train = vectorizer.fit_transform(train_texts)\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train, train_labels)\n",
        "\n",
        "# Keyword list for quick checks\n",
        "# SCAM_KEYWORDS = {\n",
        "#     'urgent', 'verify', 'password', 'won', 'prize', 'click',\n",
        "#     'suspended', 'account', 'claim', 'immediately', 'link'\n",
        "# }\n",
        "\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "# from transformers import pipeline\n",
        "\n",
        "# pipe = pipeline(\"text-classification\", model=\"ealvaradob/bert-finetuned-phishing\")\n",
        "\n",
        "# Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"ealvaradob/bert-finetuned-phishing\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"ealvaradob/bert-finetuned-phishing\")\n",
        "\n",
        "# def analyze_text(text):\n",
        "    # Quick keyword check\n",
        "    # scam_score_kw = len([w for w in text.lower().split() if w in SCAM_KEYWORDS])\n",
        "\n",
        "    # # Model prediction\n",
        "    # X = vectorizer.transform([text])\n",
        "    # proba = model.predict_proba(X)[0][1]  # probability of being scam\n",
        "\n",
        "    # # Combine both approaches\n",
        "    # final_score = 0.6 * proba + 0.4 * (scam_score_kw / 5)\n",
        "    # return final_score\n",
        "\n",
        "    # result = pipe(text)[0]\n",
        "    # label = result['label']\n",
        "    # score = round(result['score'] * 100, 2)\n",
        "    # is_phishing = label.lower() == 'phishing'\n",
        "    # return {\n",
        "    #     \"score\": score,\n",
        "    #     \"verdict\": \"PHISHING\" if is_phishing else \"SAFE\",\n",
        "    #     \"explanation\": f\"Detected as {'phishing' if is_phishing else 'safe'} with confidence {score}%\"\n",
        "    # }\n",
        "\n",
        "# def check_file(file):\n",
        "#     # Read different file types\n",
        "#     text = \"\"\n",
        "#     if file.name.endswith('.pdf'):\n",
        "#         reader = PyPDF2.PdfReader(file)\n",
        "#         text = \" \".join([page.extract_text() for page in reader.pages])\n",
        "#     elif file.name.endswith('.docx'):\n",
        "#         doc = docx.Document(file)\n",
        "#         text = \" \".join([para.text for para in doc.paragraphs])\n",
        "#     elif file.name.endswith('.txt'):\n",
        "#         with open(file.name, 'r') as f:\n",
        "#             text = f.read()\n",
        "#     print(text)\n",
        "#     return analyze_text(text)\n",
        "\n",
        "# def check_audio(audio):\n",
        "#     # Convert audio to text\n",
        "#     r = sr.Recognizer()\n",
        "#     with sr.AudioFile(audio) as source:\n",
        "#         audio_data = r.record(source)\n",
        "#         try:\n",
        "#             text = r.recognize_google(audio_data)\n",
        "#             return analyze_text(text)\n",
        "#         except:\n",
        "#             return {'score':0.0, \"verdict\":'SAFE'}  # If can't convert, assume safe\n",
        "\n",
        "\n",
        "aai.settings.api_key = \"c35d80e09f2142e0aa61e0c67033ebd4\"\n",
        "transcriber = aai.Transcriber()\n",
        "def check_audio(audio):\n",
        "    transcript = transcriber.transcribe(audio)\n",
        "\n",
        "    # Alternatively, if you have a URL to an audio file, you can transcribe it with the following code.\n",
        "    # transcript = transcriber.transcribe(\"https://storage.googleapis.com/aai-web-samples/espn-bears.m4a\")\n",
        "\n",
        "    # After the transcription is complete, the text is printed out to the console.\n",
        "    # print(transcript.text)\n",
        "    # return analyze_text(transcript.text)\n",
        "    return analyze_text_with_llamaindex(transcript.text)\n",
        "\n",
        "def final_judgment(result):\n",
        "    print('-------fg-----------------', result,type(result))\n",
        "    score = result['score']\n",
        "    threshold = 0.5\n",
        "    if score > threshold and result['verdict'].lower()=='phishing':\n",
        "        return f\"⚠️ SCAM ALERT! ({(score*100):.2f}% confidence) \\n {result.get('explanation')}\"\n",
        "    else:\n",
        "        return f\"✅ Likely Safe ({100 - (score*100):.2f}%) \\n {result.get('explanation')}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FhqT5IV8xws"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import pathlib\n",
        "import httpx\n",
        "\n",
        "client = genai.Client(api_key = \"AIzaSyDKEfEmnE9C7OcJ7cSE5rEUV2OBtzXgM0I\")\n",
        "\n",
        "# doc_url = \"/content/343019_3_art_0_py4t4l_convrt.pdf\"  # Replace with the actual URL of your PDF\n",
        "\n",
        "# Retrieve and encode the PDF byte\n",
        "filepath = pathlib.Path('/content/BT_Fraud - Copy.pdf')\n",
        "# filepath.write_bytes(httpx.get(doc_url).content)\n",
        "\n",
        "prompt = \"\"\"Is this a phishing / scam ?\n",
        "Respond strictly in only the below JSON schema with no text outside schema:\n",
        "{\"score\":0-1 (score of how much safe or phishing it is), verdict:\"phishing\" or \"safe\", \"explanation\":\"why it is scam\"}\n",
        "\"\"\"\n",
        "# response = client.models.generate_content(\n",
        "#   model=\"gemini-1.5-pro\",\n",
        "#   contents=[\n",
        "#       types.Part.from_bytes(\n",
        "#         data=filepath.read_bytes(),\n",
        "#         mime_type='application/pdf',\n",
        "#       ),\n",
        "#       prompt])\n",
        "# print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIsr0lUzCnJ4",
        "outputId": "9364719a-267b-48d8-d94a-516bd40aa5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://2bad-35-196-30-149.ngrok-free.app\n",
            "\n",
            "API Endpoints:\n",
            "- Text: https://2bad-35-196-30-149.ngrok-free.app/api/text\n",
            "- File: https://2bad-35-196-30-149.ngrok-free.app/api/file\n",
            "- Audio: https://2bad-35-196-30-149.ngrok-free.app/api/audio\n",
            "UI Interface: https://2bad-35-196-30-149.ngrok-free.app/ui\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !pip install gradio scikit-learn speechrecognition pydub PyPDF2 python-docx pyngrok nest-asyncio fastapi uvicorn python-multipart\n",
        "\n",
        "import gradio as gr\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import speech_recognition as sr\n",
        "import os\n",
        "import PyPDF2\n",
        "import docx\n",
        "import threading\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Add CORS support\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class TextInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# --- API Endpoints ---\n",
        "\n",
        "@app.post(\"/api/hello\")\n",
        "async def api_hello(name: TextInput):\n",
        "    return {\"message\": f\"Hello, {name.text}!\"}\n",
        "\n",
        "@app.post(\"/api/text\")\n",
        "async def api_text(input: TextInput):\n",
        "    # result = analyze_text(input.text)\n",
        "    result = analyze_text_with_llamaindex(input.text)\n",
        "    return result.model_dump()['raw']\n",
        "    # return {\"score\": float(result['score']), \"verdict\": final_judgment(result)}\n",
        "\n",
        "# @app.post(\"/api/file\")\n",
        "# async def api_file(file: UploadFile = File(...)):\n",
        "#     score = check_file(file.file)\n",
        "#     return {\"score\": float(score), \"verdict\": final_judgment(score)}\n",
        "\n",
        "@app.post(\"/api/file\")\n",
        "async def api_file(file: UploadFile = File(...)):\n",
        "    # Save uploaded file to a temporary location\n",
        "    suffix = \".\" + file.filename.split(\".\")[-1]\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "        shutil.copyfileobj(file.file, tmp)\n",
        "        tmp_path = tmp.name\n",
        "    result = analyze_pdf_with_llamaindex(tmp_path)\n",
        "    print('-----------score------------------',result)\n",
        "    return result\n",
        "    # return {\"score\": float(result['score']), \"verdict\": final_judgment(result), \"explanation\": result['explanation']}\n",
        "    # return score\n",
        "\n",
        "@app.post(\"/api/audio\")\n",
        "async def api_audio(audio: UploadFile = File(...)):\n",
        "    result = check_audio(audio.file)\n",
        "    return result\n",
        "    # return {\"score\": float(result['score']), \"verdict\": final_judgment(result), 'explanation': result['explanation']}\n",
        "    # return {\"score\": float(score), \"verdict\": final_judgment(score)}\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the ScamShield API!\"}\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "with gr.Blocks(title=\"ScamShield\") as demo:\n",
        "    gr.Markdown(\"# 🛡️ ScamShield AI Detector\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"📝 Text Input\"):\n",
        "            text_input = gr.Textbox(label=\"Enter suspicious text\")\n",
        "            text_output = gr.Label()\n",
        "            text_btn = gr.Button(\"Analyze Text\")\n",
        "\n",
        "        with gr.TabItem(\"📁 File Upload\"):\n",
        "            file_input = gr.File(label=\"Upload document (PDF, DOCX, TXT)\")\n",
        "            file_output = gr.Label()\n",
        "            file_btn = gr.Button(\"Analyze File\")\n",
        "\n",
        "        with gr.TabItem(\"🎙️ Audio Input\"):\n",
        "            audio_input = gr.Audio(label=\"Record/Upload suspicious call\", type=\"filepath\")\n",
        "            audio_output = gr.Label()\n",
        "            audio_btn = gr.Button(\"Analyze Audio\")\n",
        "\n",
        "    text_btn.click(\n",
        "        # fn=lambda x: final_judgment(analyze_text(x)),\n",
        "        fn=lambda x: final_judgment(analyze_text_with_llamaindex(x).model_dump()['raw']),\n",
        "        inputs=text_input,\n",
        "        outputs=text_output\n",
        "    )\n",
        "    file_btn.click(\n",
        "        fn=lambda x: final_judgment(analyze_pdf_with_llamaindex(x)),\n",
        "        inputs=file_input,\n",
        "        outputs=file_output\n",
        "    )\n",
        "    audio_btn.click(\n",
        "        fn=lambda x: final_judgment(check_audio(x).model_dump()['raw']),\n",
        "        inputs=audio_input,\n",
        "        outputs=audio_output\n",
        "    )\n",
        "\n",
        "# --- Colab Setup ---\n",
        "# nest_asyncio.apply()\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# app.mount(\"/ui\", demo)\n",
        "\n",
        "# Start server in a thread\n",
        "config = uvicorn.Config(\n",
        "        app=app,\n",
        "        host=\"0.0.0.0\",\n",
        "        port=8000,\n",
        "        log_level=\"info\"\n",
        "    )\n",
        "def run_server():\n",
        "    server_running = True\n",
        "    # server = uvicorn.Server(config)\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "    # while server_running:\n",
        "\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# --- To Stop the Server ---\n",
        "def stop_server():\n",
        "    global server_running\n",
        "    server_running = False\n",
        "    ngrok.kill()\n",
        "    print(\"Stopping server...\")\n",
        "    # server_thread.join()\n",
        "    print(\"Server stopped\")\n",
        "\n",
        "# Mount Gradio interface\n",
        "\n",
        "# Display links\n",
        "print(f\"\"\"\n",
        "API Endpoints:\n",
        "- Text: {ngrok_tunnel.public_url}/api/text\n",
        "- File: {ngrok_tunnel.public_url}/api/file\n",
        "- Audio: {ngrok_tunnel.public_url}/api/audio\n",
        "UI Interface: {ngrok_tunnel.public_url}/ui\n",
        "\"\"\")\n",
        "\n",
        "# demo.launch(share=True, debug=True)\n",
        "# To stop the server later, run:\n",
        "# stop_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqSbPJXSCubA",
        "outputId": "f0f1f3ff-bf1f-41f2-f31a-9edaa9508012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping server...\n",
            "Server stopped\n"
          ]
        }
      ],
      "source": [
        "stop_server()\n",
        "# server_thread.join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fv7iUTuLVHI",
        "outputId": "e45e2ace-d8c7-45cf-a460-33eb642e19cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Active threads:\n",
            "MainThread\n",
            "Thread-2\n",
            "_colab_inspector_thread\n",
            "Thread-8 (start_server)\n",
            "Thread-13 (run_server)\n"
          ]
        }
      ],
      "source": [
        "# List all active threads\n",
        "import threading\n",
        "active_threads = threading.enumerate()\n",
        "print(\"Active threads:\")\n",
        "for thread in active_threads:\n",
        "    print(thread.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "kplDxvfDrmj8",
        "outputId": "83206008-5496-4a50-8731-045befc13227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------text-res-------------------------------- {\"score\":0.1,\"verdict\":\"safe\",\"explanation\":\"The provided text 'asasas' lacks typical scam indicators such as urgent requests, suspicious links, poor grammar, unusual sender addresses, threats, or financial requests. It appears to be a random string with no clear context or malicious intent.\"} <class 'llama_index.core.base.llms.types.CompletionResponse'>\n",
            "INFO:     127.0.0.1:49454 - \"POST /api/text HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "res = requests.post(\"http://localhost:8000/api/text\", json={\"text\":'asasas'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6a3Tgt13dh4",
        "outputId": "1bfd5f79-8e55-40a4-c6ab-5e3ee1302638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.1,\n",
              " 'verdict': 'safe',\n",
              " 'explanation': \"The provided text 'asasas' lacks typical scam indicators such as urgent requests, suspicious links, poor grammar, unusual sender addresses, threats, or financial requests. It appears to be a random string with no clear context or malicious intent.\"}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "pi0pHdYBssrN",
        "outputId": "51facf50-eeea-4136-d66e-2874ba1d61c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------score------------------ {'score': 0.7, 'verdict': 'phishing', 'explanation': \"The email contains a suspicious link (britishair.io/login) and an unexpected financial offer (a £300 British Airways travel voucher). It also mentions a disclaimer about the voucher's terms and conditions, which is a common tactic in scams.\"}\n",
            "INFO:     127.0.0.1:41144 - \"POST /api/file HTTP/1.1\" 200 OK\n",
            "200\n",
            "{'score': 0.7, 'verdict': 'phishing', 'explanation': \"The email contains a suspicious link (britishair.io/login) and an unexpected financial offer (a £300 British Airways travel voucher). It also mentions a disclaimer about the voucher's terms and conditions, which is a common tactic in scams.\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-0b200d76b3c0>:97: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  return response.response.dict()\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://localhost:8000/api/file\"\n",
        "file_path = \"/path/to/your/file.pdf\"  # 🔁 Replace with actual path\n",
        "\n",
        "with open(\"/content/BT_Fraud - Copy.pdf\", \"rb\") as f:\n",
        "    files = {\"file\": (\"/content/BT_Fraud - Copy.pdf\", f, \"application/pdf\")}\n",
        "    response = requests.post(url, files=files)\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "WNul-K48u9LO",
        "outputId": "be89eb5a-d116-4286-92c4-a7db2a6dcd2d"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-10ff9f9f2ed9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import signal\n",
        "\n",
        "os.kill(os.getpid(), signal.SIGINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvzGQ9g610HM",
        "outputId": "0a978518-2417-4d60-d94d-6ae20d427393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMAND    PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n",
            "python3 128777 root   55u  IPv4 3212099      0t0  TCP *:8000 (LISTEN)\n",
            "python3 128777 root   57u  IPv4 3229804      0t0  TCP localhost:51906->localhost:8000 (CLOSE_WAIT)\n",
            "python3 128777 root   68u  IPv4 3221122      0t0  TCP localhost:53496->localhost:8000 (CLOSE_WAIT)\n",
            "python3 128777 root   70u  IPv4 3222412      0t0  TCP localhost:60318->localhost:8000 (CLOSE_WAIT)\n"
          ]
        }
      ],
      "source": [
        "!sudo lsof -i TCP:8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOCQ2pre7-aF"
      },
      "outputs": [],
      "source": [
        "!kill -9 128777"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "aBZTLY0f88bd",
        "outputId": "173e5f12-0f50-4dc9-bea2-4fbf5b725b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d4be620bd40d0ab488.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d4be620bd40d0ab488.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------text-res-------------------------------- {\"score\":0.9,\"verdict\":\"phishing\",\"explanation\":\"The text contains a direct and unexpected financial request, coupled with poor grammar and spelling. This combination of factors indicates a high likelihood of a scam.\"} <class 'llama_index.core.base.llms.types.CompletionResponse'>\n",
            "-------fg----------------- {'score': 0.9, 'verdict': 'phishing', 'explanation': 'The text contains a direct and unexpected financial request, coupled with poor grammar and spelling. This combination of factors indicates a high likelihood of a scam.'} <class 'dict'>\n",
            "-----------text-res-------------------------------- {\"score\":0.9,\"verdict\":\"phishing\",\"explanation\":\"The text contains unusual sender addresses, unexpected financial requests, and pressure tactics. The sender is offering a job with high daily pay and immediate payment, which is unusual. The message also includes a Telegram contact, which is not a typical way for companies to communicate with potential employees.\"} <class 'llama_index.core.base.llms.types.CompletionResponse'>\n",
            "-------fg----------------- {'score': 0.9, 'verdict': 'phishing', 'explanation': 'The text contains unusual sender addresses, unexpected financial requests, and pressure tactics. The sender is offering a job with high daily pay and immediate payment, which is unusual. The message also includes a Telegram contact, which is not a typical way for companies to communicate with potential employees.'} <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4a4WRNIWtj-",
        "outputId": "2836b82b-2982-4612-9fa7-2c1e4e2228e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TseFA3D9XHn9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}